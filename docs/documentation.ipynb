{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation: \"KeyDifferentiator\" Auto-Diff Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** Kate Grosch, Mingyue Wei, Spencer Penn\n",
    "\n",
    "**Summary:** Automatic Differentiation Library created in CS207 course at Harvard University taught by Dr. David Sondak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal in this project is to develop a software library for Automatic Differentiation. We created a useful tool, familiarized ourselves with software development best-practices, and learned how machines perform differentiation.\n",
    "\n",
    "The final deliverable is an automatic differentiation library in python that is easy to understand, install, and use -- including clear documentation and testing.\n",
    "\n",
    "Beyond the baseline expectation, we extend our project in a couple of key specific ways: namely the inclusion of Reverse-Mode automatic differentiation (explained below) as well as string-parsing functionality in terminal for simple functions. \n",
    "\n",
    "**Extended Functionality:** The basic automatic differentiation accepts values or vectors to represent the functional inputs. Beyond the basic functionality, our library includes two key feature extensions. The first is (1) string parsing. The user of the library in terminal will be able to input a function as a text-entry into the command line. Of course, the functionality still exists for users to access the underlying `AD` library directly. And secondly (2), we are extending the functionality to include both forward-mode as well as reverse-mode automatic differentiation accumulation (explained below). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation is how computers can automatically evaluate derivatives, which comes up in a lot of contexts. The two most obvious approaches to differentiation are symbolic differentiation (applying the rules of variable-level differentiation in software) and numerical differentiation (a finite difference-based approximation of a derivative). \n",
    "\n",
    "The failure of symbolic differentiation is that it can be inefficient in solving high-complexity expressions, yielding long computation times. By contrast, the failure of numerical differentiation is that it can be prone to floating-point error in discretization or suffer from numerical instability.\n",
    "\n",
    "### The Chain Rule\n",
    "In contrast, automatic differentiation applies the chain rule repeatedly to break down arbitrarily complicated differentiation problems into a series of elementary arithmetic operations and elementary functions. \n",
    "\n",
    "$$ F'(x) = f'(g(x))g'(x)$$\n",
    "\n",
    "In a way, a derivative is a local linear approximation of a function. By applying the chain rule, we can break down any derivative solving problem, even with high complexity or a partial derivative with respect to many inputs (e.g. gradient-based optimization) into a sequential composition of functions.  \n",
    "\n",
    "### Graph Structure of Calculations\n",
    "While there are many different ways to conceptualize the process used in automatic differentiation, the one explored in our class involves a graphical representation of the calculation. \n",
    "\n",
    "In the first step, we break down the function into an “evaluation trade,” with a set of xi traces (or intermediary computed variables).\n",
    "\n",
    "From there, you can visualize the calculation steps in an “evaluation graph” - simply a graphical representation of the computation elements with edges and notes (similar to a control loop diagram). \n",
    "\n",
    "### Elementary Functions\n",
    "Finally, through each step you evaluate both the trace in terms of the input variable and also compute the derivative using the chain rule. At the end of this process, the final xi trace now is assigned both a value and a derivative that is simply composed of elemental function that are easy for a computer to evaluate. \n",
    "\n",
    "To give some concrete examples, the key elementary arithmetic operations include: addition, subtraction, multiplication, division, etc... and the key elementary functions include: exp, log, sin, cos, etc...\n",
    "\n",
    "\n",
    "### Example of Automatic Differentiation:\n",
    "\n",
    "Consider an example function:\n",
    "\n",
    "![forward.png](function.png)\n",
    "\n",
    "Looking at the partial derivative with respect to x2 is already pretty hairy: \n",
    "\n",
    "![partial.png](partial.png)\n",
    "\n",
    "To evaluate the gradient or derivative at any arbitrary value, numerical methods would be plagued with truncation and round-off errors. While the symbolic method is algebraically challenging and computationally expensive. \n",
    "\n",
    "With Automatic differentiation we can build a computational graph to compartmentalize the function into elementary sequential steps:\n",
    "\n",
    "![graph.png](graph.png)\n",
    "\n",
    "_Computational Graph for Function in the Forward Mode_\n",
    "\n",
    "To evaluate the value and derivative at each node, we walk though the graph and compose the evaluation traces:\n",
    "\n",
    "![forward.png](forward.png)\n",
    "\n",
    "In the forward trace mode, each term can be calculated from operands that have already been calculated in a higher term. \n",
    "\n",
    "The reverse mode is a similar technique, sometimes referred to as backpropagation, but instead the trace on the right is evaluated from bottom to top:\n",
    "\n",
    "![reverse.png](reverse.png)\n",
    "\n",
    "(NOTE: we were instructed by our TA, CJ Xin, to include images from the lecture slides for additional context, but to cite below)\n",
    "\n",
    "### Citations:\n",
    "We would like to attribute some of the graphics above to the following sources:\n",
    "- CS207 Course Website: https://harvard-iacs.github.io/2019-CS207/\n",
    "- A Hitchhiker’s Guide to Automatic Differentiation: Hoffmann, P.H.W. Numer Algor (2016) 72: 775. https://doi.org/10.1007/s11075-015-0067-6\n",
    "- The magic of Automatic Differentiation, Sanyam Kapoor: https://www.sanyamkapoor.com/machine-learning/autograd-magic/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use KeyDifferentiator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Installation\n",
    "To install the KeyDifferentiator automatic differentiation package, the user should run:\n",
    "```\n",
    "pip install keydifferentiator\n",
    "```\n",
    "Alternatively, the user could download and run the package using the source files on GitHub. Simply run:\n",
    "```\n",
    "git clone https://github.com/Key-Differentiators/cs207-FinalProject.git\n",
    "cd cs207-FinalProject\n",
    "python setup.py\n",
    "```\n",
    "\n",
    "We recommend that developers and consumers follow this same installation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation (forward mode)\n",
    "\n",
    "After installation, the user can use the AD library and unary library directly to accomplish forward mode automatic differentiation. Open a python file and import the two modules:\n",
    "\n",
    "```\n",
    "from keydifferentiator.AD import AD\n",
    "from keydifferentiator import unary\n",
    "```\n",
    "\n",
    "The `AD` module has the basic methods and operations, and the `unary` module holds other elementary functions such as trig functions, inverse trig functions, natural base exponential, etc.\n",
    "\n",
    "Create an AD Object, run basic operations and unary functions:\n",
    "\n",
    "```\n",
    "# basic operations\n",
    "x = AD(3.0)\n",
    "y = AD(4.0)\n",
    "a = x*y\n",
    "\n",
    "# display parameter values:\n",
    "print(a.val, a.der)\n",
    "\n",
    "# unary functions\n",
    "unary.sin(x)\n",
    "unary.ln(a)\n",
    "```\n",
    "\n",
    "In the previous demo, `x` and `y` are considered as the same variable, because they are instantiated with a single derivative by default. To initialize multiple variables, the derivatives should be specified during in instantiation. All other operations will work. See sample codes below.\n",
    "\n",
    "```\n",
    "# x,y,z are three variables\n",
    "x = AD(1.0, [1.0, 0.0, 0.0])\n",
    "y = AD(2.0, [0.0, 1.0, 0.0])\n",
    "z = AD(3.0, [0.0, 0.0, 1.0])\n",
    "\n",
    "a = x*y\n",
    "print(a.der)\n",
    "```\n",
    "\n",
    "The returned derivative will be a numpy array holding partial derivatives of `x`, `y`, and `z` with respect to function `a`.\n",
    "\n",
    "The `AD` module also has a special function `get_jacobian` for getting the Jacobian matrix for a vector of functions. Suppose the `x`, `y`, and `z` are instantiated as above, one can input the following code:\n",
    "\n",
    "```\n",
    "f1 = x*y\n",
    "f2 = z+2\n",
    "vals, ders = AD.get_jacobian([f1,f2])\n",
    "```\n",
    "\n",
    "where `vals` holds the 3 function values, and `ders` is a $2X3$ Jacobian matrix where the first row represents $[\\frac{\\partial \\text{f1}}{\\partial{x}},\\frac{\\partial \\text{f1}}{\\partial{y}},\\frac{\\partial \\text{f1}}{\\partial{z}}]$ and the second row represents the partial derivatives with respect to function f2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Features\n",
    "\n",
    "#### Using The Command-Line Interface (String-Parser)\n",
    "\n",
    "Users interact with our module via a command-line interface. To start the interface, run:\n",
    "```\n",
    "python -m keydifferentiator\n",
    "```\n",
    "The command line will prompt the user to enter a function. The tool only accepts single-variable functions. Here are some examples:\n",
    "```\n",
    "Input your function:   f(x) = 3*x + 4\n",
    "Input your function:   f(x) = cos(x) + x**2\n",
    "Input your function:   f(x) = 3*x + 4\n",
    "Input your function:   f(x) = 3*x**2 + 2*x + 1\n",
    "```\n",
    "Once the user enters a function, the user is prompted to enter a value of x at which to evaluate the derivative. Here is an example:\n",
    "```\n",
    "What value of x would you like to evaluate at? Enter a number: 3\n",
    "```\n",
    "The interface then prints out the values of the function f(x) and the derivative f'(x) at that value of x:\n",
    "```\n",
    "f(x)=34.00, f'(x)=20.00\n",
    "```\n",
    "The user is then prompted to enter another function (if desired) or enter `q` to quit.\n",
    "\n",
    "\n",
    "#### Reverse-Mode Automatic Differentation\n",
    "\n",
    "Example of how to use the Reverse-Mode Automatic Differentiation:\n",
    "\n",
    "```\n",
    "f = 4 * Reverse(3) - 6\n",
    "f.gradient_value = 1 #set gradient value of function to get derivatives of variables\n",
    "f.value #should be 6\n",
    "x.get_gradient() #should be 4\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directory Structure\n",
    "\n",
    "Our directory structure is outlined below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "/cs207-FinalProject\n",
    "\tREADME.md\n",
    "\tLICENSE\n",
    "    .gitignore\n",
    "    .travis.yml\n",
    "    setup.py\n",
    "    requirements.txt\n",
    "    docs/\n",
    "        documentation.ipynb\n",
    "        milestone1.ipynb\n",
    "        milestone2.ipynb\n",
    "    keydifferentiator/\n",
    "        __init__.py\n",
    "        __main__.py\n",
    "        AD.py\n",
    "        unary.py\n",
    "        Reverse.py\n",
    "    tests/\n",
    "        __init__.py\n",
    "        AD_test.py\n",
    "        unary_test.py\n",
    "        rev_AD_test.py\n",
    "        diff_test.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **README.md**: Project documentation, build status, and code coverage.\n",
    "- **LICENCE**: Text of the MIT license.\n",
    "- **.gitignore**: Files to exclude when pushing to GitHub.\n",
    "- **.travis.yml**: Configures the TravisCI build integration.\n",
    "- **setup.py**: Packages the modules into the keydifferentiator package.\n",
    "- **requirements.txt**: Describes version and installation requirements of the build.\n",
    "- **docs/**: Directory containing Milestone 1 and Milestone 2 documentation.\n",
    "- **keydifferentiator/**: \n",
    "    - **__init__.py**: Tells setup.py that this is a package.\n",
    "    - **__main__.py**: Contains the command-line interface for the project (including string-parsing)\n",
    "    - **AD.py**: Contains the AD class, and overwrites dual operations for AD objects.\n",
    "    - **unary.py**: Contains a library of unary operations for AD objects.\n",
    "    - **Reverse.py**: Contains a library of reverse mode AD\n",
    "- **tests/**:\n",
    "    - **__init__.py**: Tells setup.py that this is a package.\n",
    "    - **AD_test.py**: Test code for the AD class.\n",
    "    - **unary_test.py**: Test code for the unary function library.\n",
    "    - **rev_AD_test.py**: Test code for reverse mode AD\n",
    "    - **diff_test.py**: Test code for string parsing evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Choice\n",
    "\n",
    "\n",
    "#### PyPi\n",
    "To speak directly to our packaging of this software library, the team chose to distribute through PyPi - The Python Package Index - the official third-party software repository for Python\n",
    "\n",
    "Our specific distribution can be found here: https://pypi.org/project/keydifferentiator/\n",
    "\n",
    "The PyPi repository boasts 205,185 projects, across 1,544,442 releases, or 2,302,428 files, employed by 386,601 users. Distributing here poses several benefits. Beyond being a standard and widely-used method for publishing libraries, PiPy also allows our uses to employ the pip install method as directed above in the \"Installation\" section. \n",
    "\n",
    "Here we can maintain the project, its descriptions as well as a release history to give transparency and maintainability to users. \n",
    "\n",
    "#### PIP Install\n",
    "\n",
    "By distributing through PyPi, we also enable our users to install the library via PIP. PIP is the de-facto standard package-management system used to install and manage software packages written in Python.\n",
    "\n",
    "#### Note on Dependences: \n",
    "Many libraries (including ours) have dependencies on other standard libraries (e.g. in our case, primarily `numpy`). By including a requirements.txt file in the package distribution through PyPi and employing PIP for installation -- the system also installs all dependencies so long as they are specified properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Tests live within `AD_test.py` and `unary_test.py` files. \n",
    "\n",
    "For maintaining real-time testing at a high standard we use both Travis CI and Codecov. Travis CI is ahosted continuous integration service used to build and test software projects. While Codecov gives a metric for the portion of the library code which is covered by our tests. \n",
    "\n",
    "Testing status for both approaches is viewable on our Github and PyPi repository pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the Hood\n",
    "To implement the forward mode of automatic differentiation, we will create a main class called `AD`. The class will be initiated with two attributes: `val` (for value) and `der` (for derivative), while `val` is required and `der` is optional. The default value for `der` is a length one list of value 1.0. \n",
    "```\n",
    "class AD():\n",
    "\tdef __init__(val, der=[1.0]):\n",
    "\t\tself.val = val\n",
    "\t\tself.der = np.array(der)\n",
    "```\n",
    "\n",
    "As the pseudo-code represents, the `val` of the AD object will be set to the given value, and the `der` will be set to a numpy array. This is for easier manipulation when multivariate operations are involved because numpy array is more flexible in handling operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard dunder methods are re-written within the `AD` class: `__add__`, `__radd__`, `__mul__`, `__rmul__`, `__sub__`, `__rsub__`, `__truediv__`, `__rtruediv__`, `__pow__`, `__rpow__`, and `__neg__`.\n",
    "\n",
    "Two comparison methods are also implemented `__eq__` and `__ne__`, and the comparison rule is as following:\n",
    "1. If both variables are AD objects, they must be equal in both `val` and `der` to be considered equal\n",
    "2. If an AD object is compared to a numeric value, i.e. a float, they are considered equal if `val` of the AD object equals to the numeric value\n",
    "\n",
    "A special method for getting the Jacobian matrix is also implemented: `get_jacobian`\n",
    "\n",
    "Within the helper library `unary.py`, other elementary functions are implemented, and they include:\n",
    "* Exponentials: `exp`\n",
    "* Logarithms: `ln`, `log`\n",
    "* Logistics: `logistic`\n",
    "* Square root: `sqrt`\n",
    "* Trig functions: `sin`, `cos`, `tan`\n",
    "* Inverse trig functions: `arcsin`, `arccos`, `arctan`\n",
    "* Hyperbolic functions: `sinh`, `cosh`, `tanh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unary Structure in More Detail\n",
    "\n",
    "Within the helper library `unary.py`, we outline a number of elementary functions that are used in the process of automatic differentiation. \n",
    "\n",
    "Looking at `sin` as one specific example:\n",
    "\n",
    "```\n",
    "def sin(x):\n",
    "\ttry:\n",
    "\t\treturn ad.AD(np.sin(x.val), x.der * np.cos(x.val))\n",
    "\texcept AttributeError:\n",
    "\t\treturn np.sin(x)\n",
    "```\n",
    "\n",
    "Each unary function is set up in a similar way, taking in an `AD` object parameter values and then applying a specific elementary operation to return a resulting object. `try-except` block is used to check for input type and handle non-AD object scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User perspective\n",
    "\n",
    "User could interact with KeyDifferentiator package in two approaches, and both were given sample codes under **How to Use KeyDifferentiator** section:\n",
    "\n",
    "1. Interact directly with the `AD` library: create AD object and do basic operations like `add, sub, multiply`, etc. and other elementary functions like `exp, ln, sin, cos`, etc.\n",
    "\n",
    "2. Use keydifferentiator through Command-Line interface for single variable evaluation: input a string representation of a function to differentiate. The module will then parse the string into an `AD` object, and call methods in the `AD` module to get the `val` and `der` of the object.\n",
    "\n",
    "The dependencies for this packages are: numpy, math, regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The basic automatic differentiation accepts values or vectors to represent the functional inputs. As mentioned in the intro, beyond the basic functionality, our library includes two key feature extensions: (1) string parsing, and (2) both _forward-mode_ as well as *reverse-mode* automatic differentiation accumulation.\n",
    "\n",
    "#### Reverse-Mode Auto Diff:\n",
    "\n",
    "The reverse mode automatic differentiation is similar to the forward-mode (described above), sometimes referred to as backpropagation, but instead the trace on the right is evaluated from bottom to top.\n",
    "\n",
    "First we define the adjoint of a variable 'vi' as 'dy/dvi'. For intuition, this variable describes the sensitivity of the output with respect to the immediate variable vi. Then from here, we build the Reverse Adjoint Trace: \n",
    "\n",
    "![reverse.png](reverse.png)\n",
    "\n",
    "\n",
    "Through this single pass, the trace provides the output with respect to each input variable, evaluated from bottom to top. Here the lower term is calculated before the higher term - visually you can imagine traversing the computational flow graph in reverse order. \n",
    "\n",
    "\n",
    "For further reading: the magic of Automatic Differentiation, Sanyam Kapoor: https://www.sanyamkapoor.com/machine-learning/autograd-magic/\n",
    "\n",
    "\n",
    "\n",
    "#### Other Extended Elements:\n",
    "\n",
    "Worth noting, beyond string parsing (user inputting a function as a text-entry into the command line), users can still directly access and call the underlying AD library.\n",
    "\n",
    "Already today, by calling `python -m keydifferentiator` the user can already enter functions in a plain-text format. However, to extend this text parsing even farther, we update our libary accepts more complex equations as well as vectorized notation. \n",
    "\n",
    "Moreover, we implemented the reverse-mode accumuation approach to AD. \n",
    "\n",
    "#### Future Extensions:\n",
    "\n",
    "And finally, in a couple months, we will go even yet one step further to auto-generate graphs of the underlying function, the values, and the derivative function with useful labeling for users. These auto-generated plots will likely rely on the `matplotlib` library. This visual representation we hope to give additional intuition to the users on the functions they plan to evaluate. We hope to maintain this library going forward, but this feature we plan for a future release. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
